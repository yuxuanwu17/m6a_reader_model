## Material and Methods
### Identification of m6A reader binding sites

For the benefits of modeling, we need to define the positive and negative samples of m6A reader binding site. As can be seen from Figure 1, there are three significant factors to consider when determining the m6A reader binding sites, which is DRACH motif, known m6A Sites and CLIP labeled Sites. Generally, m6A readers have a tendency to bind known m6A sites and DRACH motif, but these two are not robust enough, without CLIP labeled sites, they could still be considered as negative samples. Therefore, in this experiment, three factors have to be satisfied simultaneously. In addition, the position of gene model would not influence the samples. To minimize the bias in selecting the polyA RNAs, we prepared the full transcript data and mature RNA data. In detail, mature RNA data exclude the sites on the intron region while the full transcript data covered either the exon or intron region.

![Alt text](https://github.com/yuxuanwu17/m6A_reader/blob/master/plot/m6a_criteria.png) 

**Figure 1** Criteria of determining the m6A reader binding sites.

### Deep learning model construction

Previously, we conducted a traditional machine learning about m6A reader by different encoding methods, however, the performance of one-hot method was not ideal, there was still a gap to improve. In addition, one-hot encoding method learned from the convolutional neural network (CNN) is suitable for learning potential motifs in the bioinformatics field, therefore, we opted for deep learning techniques in this research. Furthermore, recurrent neural network (RNN) was incorporated since it could capture the information in sequence, for instance, the potential relationship between each nucleotide. 

To build the deep learning model, we used Keras v2.3.0 and R v4.0.2 to conduct the learning part and process the raw data for prediction. For the data preparation part, we used R to extract n nucleotides (bp) of flanking sequences centered on the target adenosine, ranging from 251 to 2001bp to explore a suitable length. The processed sequence data were then inputted to Python3 for encoding, in this case, we chose One-hot encoding method for better model interpretability, for instance, A (1,0,0,0), C (0,1,0,0), G (0,0,1,0), T (0,0,0,1). The overall framework can be seen in Figure 2. Each sequence was then transformed to an n×4 matrix and fed into two combinations of 1D convolution (Conv1D) layer and max-pooling layer. For the first combination, we set 90 kernels with size equaled 5 and applied L2 regulation to prevent overfitting. The rectified linear unit (ReLU) was used as the activation function to provide our necessary non-linearity. The following max-pooling layer was set in size equaled 4 with strides 2 to reduce the dimension of output from the previous layer. The dropout rate was incorporated to 0.25 to further reduce the possibility of overfitting. A second 1D convolution (Conv1D) layer with 100 filters and size equaled 3 to extract the feature of the previous data. Similarly, the ReLU function and L2 regulation were applied. However, the max-pooling size was 10 with 1 stride, under which circumstance could the model achieve higher performance. 

The recurrent neural networks long short-term memory (LSTM) layer was used to aggregate the outputs of CNNs for predicting the RBP binding, in this case, the m6A readers’ substrates sites. LSTM processed sequentially of the sequence element, hoping to capture the inter-dependencies between motifs. Moreover, the fully connected layer with 1000 neurons would receive the output from the LSTM layer, and the non-linear activation function n, sigmoid, would calculate the prediction probability in each training class. The overall tuning process was used the loss function, binary cross-entropy to conduct the weight-tuning, optimizing the learning process, additionally, we found that Adam is the most suitable for this task. Finally, the output would be the probability of being m6A reader substrate sites. 

![Alt text](https://github.com/yuxuanwu17/m6A_reader/blob/master/plot/Architecture(3).png) 

**Figure 2** The sequence data are encoded by One-hot method and fed into the convolution layer and followed by the pooling layer twice to extract the significant features. The LSTM layer learns the long-term dependencies between sequence data generated by convolution layers. The flatten layer combines the previous kernels into a vector and inputs to the fully connected layer to calculate the probability of being m6A reader substrate site 

### Training strategy and performance evaluation

We separated each gene data set into three categories, training, testing, validation dataset, the ratio was 8:1:1 respectively. Moreover, to reduce the bias caused by imbalanced data samples, we ensured the same number of positive and negative samples in each category. The early stopping method was included to reduce the unnecessary computation during the learning process and the patience was designed as 10. The loss plot was drawn to document the training procedure and monitor the potential overfitting.  

To validate the model performance, four commonly used performance metrics, including area under the ROC curve (AUC), area under the Precision-Recall curve (PR-AUC), accuracy (ACC) and Mathew’s correlation coefficient (MCC). The formula of ACC and MCC are demonstrated as follows:

![Alt text](https://github.com/yuxuanwu17/m6A_reader/blob/master/plot/equation.png) 

where TP and TN are denoted as True Positive and True Negative, FN and FT are denoted as False Negative and False Positive. To sum up, the higher the performance metrics value, the more accurate the prediction. Additionally, we compared the performance with the previous research using machine learning method, the combination of CNN + RNN frameworks and the CNN framework only to determine the optimal choice.

We also exploited DeepExplain's epsilon-LRP method (gradient-based) to calculate the contribution in each feature input. With the assistance of this approach, we could rank the nucleotides’ significance in identifying the m6A readers’ substrates. Moreover, we extend the sequence upstream/downstream length from 50bp to 250bp, hoping to cover more information in determining each nucleotide contribution. 
